#' Read a shapefile containing NHDPlus HR lines
#' 
#' @description 
#' `read_ndhr_lines` imports a shapefile containing NHDPlus HR flowline data
#' and prepares it for processing in the RShade process. The preparations steps
#' performed by this function are:
#' * `sf::st_read()` - shapefile is imported as a dataframe of class "sf"
#' * `sf::st_zm()` - all Z and M values are removed
#' * `sf::st_transform()` - ensures that the dataset is in NAD83 Conus ALbers (EPSG:5070)
#' * `asp_all()` - aspect is calculated for every reach. This is done now for convenience. 
#' 
#' @param nhdplus_shapefile While a shapefile is recommended, this input can 
#' technically be any format accepted by the `sf::st_read()` function.
#' The primary requirements here is that the dataset must be lines 
#' (not points or polygons), and must include a column containing the NHDPLusID
#' of every reach as text (not numeric). Future versions of this function will
#' convert input NHDPlusID from numeric to text, as well as allow the user to 
#' specify the NHDPlusID column name.


read_nhdplus_lines <- function(nhdplus_data, nhdplus_feature_name) {
  message("Reading NHDPlus HR Dataset")
  type <- tools::file_ext(nhdplus_data)
  
  if(type == "gdb"){
    data <- st_read(nhdplus_data, layer = nhdplus_feature_name) %>%
      st_zm() %>%
      st_transform(crs = 5070)
  }
 
  if(type == "shp"){
      data <- st_read(nhdplus_data) %>%
        st_zm() %>%
        st_transform(crs = 5070)
  }
  
  if(("NHDPlusIDt" %in% colnames(data)) == FALSE){
    
    data <- transmute(data, NHDPlusIDt = as.character(NHDPlusID))
  }
  
  
  
  output <- asp_all(data) %>%
    st_cast('LINESTRING')
  
  return(output)
}


#' Generate shade sample points along NHDPlus HR lines
#' 
#' @description
#' Generates sample points at 50 meter intervals along NHDPlus HR lines and 
#' assigns a site_id to each point. 
#' 
#' @param lines Needs to be a dataframe of class "sf" containing linestring
#' geometry. lines must be in projected in NAD Conus Albers (EPSG:5070).


sample_shade_points <- function(lines, distance) {
 message("Dissolving Flowlines")
 
 nhdplus_dissolve <- st_combine(lines)
 
 sp_dissolve <- as(nhdplus_dissolve, "Spatial")
 
 message("Generating Shade Points")
 
 shade_num <- as.integer(round(st_length(nhdplus_dissolve) / distance, 0))
 
 shade_points <- spsample(sp_dissolve, shade_num, "regular") %>%
   st_as_sf()
   
 shade_points$site_id <- seq.int(nrow(shade_points))
   
 
   
 return(shade_points)
}


#' Retrieves the 4-digit Hydrologic Unit Code (HUC4) that the input lines exist within.
#' 
#' @description
#' Generates a vector containing the HUC4 numbers that the combined NHDPlus line
#' input exists within. This can be 1 or more values.  
#' 
#' @param lines Needs to be a dataframe of class "sf" containing linestring
#' geometry. lines must be in projected in NAD Conus Albers (EPSG:5070).
#' 
#' @noRD


get_huc4 <- function(lines) {
  nhdplus_dissolve <- st_combine(lines)

  huc <- st_filter(huc4_boundaries, nhdplus_dissolve)

  huc4 <- huc$huc4

  return(huc4)
}


#' Download geodatabases that correspond with the HUC4 values 
#' 
#' @description
#' Downloads the geodatabases that correspond with the input HUC4 values and
#' places them in the temporary directory. Output a vector containing the path(s)
#' to the geodatabase(s).
#' 
#' @param huc4 A vector containing 1 or more HUC4 values
#' 
#' @noRD


download_NHDPlus_GDB <- function(huc4) {
  gdb_path <- c()

  for (i in huc4) {
    path <- download_nhdplushr(tempdir(), i)

    gdb_path <- c(gdb_path, (paste(path, list.files(path)[1], sep = "/")))
  }


  return(gdb_path)
}


#' Extracts the Value Added Attributes (VAA) from the NHDPlus HR geodatabases
#' 
#' @description
#' Takes the NHDPlusFlowlineVAA table from the NHDPlus HR geodatabase and creates
#' a dataframe containing the NHDPlusID and TotDASqKM fields. If there are
#' multiple geodatabases, the NHDPlusFlowlineVAA tables are combined.
#' 
#' @param gdb_path a vector containing the path(s) to the NHDPlus HR geodatabases
#' that correspond to the HUC4s generated by `get_huc4()`
#'
#' @noRD

assign_vaa <- function(gdb_path) {
  vaa <- st_read(gdb_path[1], layer = "NHDPlusFlowlineVAA")

  vaa_TotDaSqKM <- as.data.frame(cbind(vaa$NHDPlusID, vaa$TotDASqKm))
  
  colnames(vaa_TotDaSqKM) <- c("NHDPlusID", "TotDASqKM")
  


  if (length(gdb_path) > 1) {
    for (i in 2:length(gdb_path)) {
      vaa2 <- st_read(gdb_path[i], layer = "NHDPlusFlowlineVAA")
      vaa_TotDaSqKM2 <- as.data.frame(cbind(vaa$NHDPlusID, vaa$TotDASqKm))
      colnames(vaa_TotDaSqKM2) <- c("NHDPlusID", "TotDASqKM")
      
      vaa_TotDaSqKM <- rbind(vaa_TotDaSqKM, vaa_TotDaSqKM2)
    }
  }

  return(vaa_TotDaSqKM)
}

#' Performs a spatial join between sf point and linestring geometry. 
#' 
#' @description
#' Joins lines to points enabling the transfer of information between the two. 
#' This function in necessary because the sample shade points do not perfectly
#' overlap with the NHDPlus HR lines. This function creates a small buffer around
#' the lines to compensate for this. 
#' 
#' @param points Needs to be a dataframe of class "sf" containing point
#' geometry. lines must be in projected in NAD Conus Albers (EPSG:5070).
#' 
#' @param lines Needs to be a dataframe of class "sf" containing linestring
#' geometry. lines must be in projected in NAD Conus Albers (EPSG:5070).
#' 
#' @param left.join Designate whether the join will be a left join or not.
#' Default value is TRUE
#' 
#' @param id.field Designate the name of the unique ID field within the points
#' dataset. This is used to account for cases where a point intersects 2 line
#' buffers. Default value is "site_id".
#'
#' @noRD

join_points_lines <- function(points, lines, left.join = TRUE, id.field = "site_id") {
  lines_buffer <- st_buffer(lines, 0.01)

  join_points <- st_join(points, lines_buffer, left = left.join)

  output <- join_points[!duplicated(join_points[[id.field]]), ]

  return(output)
}


#' Calculates the bankfull width at every shade sample point. 
#' 
#' @description
#' Calculates the bankfull width of every shade sample point using 2 methods.
#' The first method uses formulas from Development of Regional Curves for 
#' Hydrologic Landscape Regions (HLR) in the Contiguous United States 
#' (Blackburn-Lynch et al. 2017). The second method calculates the width of 
#' NDH_Area polygons located in the NHDPlus HR geodatabase where shade sample
#' points intersect. 
#' 
#' @param shade_points Needs to be a dataframe of class "sf" containing point
#' geometry. Points must be in projected in NAD Conus Albers (EPSG:5070) and have 
#' a unique ID field called "site_id".
#' 
#' @param nhdplus_lines Needs to be a dataframe of class "sf" containing linestring
#' geometry. Lines must be in projected in NAD Conus Albers (EPSG:5070) and have 
#' a unique ID field called "site_id".
#' 
#' @param hlr shapefile containing hydrologic landscape region areas with 
#' coefficients and exponets for the BFW calculation equation described in
#' Blackburn-Lynch et al. 2017 .
#' 
#' @param use.transect Designate whether you want to measure NHD_Area transect
#' widths in addition to using an equation to calculate bankfull width.
#' Default value is TRUE.



calc_BFW <- function(shade_points, nhdplus_lines, hlr, bfw_table, use.transect = TRUE) {
 
  
   
  if (bfw_table != FALSE){
    joinPoints <- join_points_lines(shade_points, nhdplus_lines)
    output <- copy_BFW(joinPoints, bfw_table)
    
    return(output)
  } else{
    
    huc <- get_huc4(nhdplus_lines)
    
    message(paste("HUC4: ", huc, " ", sep = ""))
    
    gdb_path <- download_NHDPlus_GDB(huc)
    
    vaa <- assign_vaa(gdb_path)
    
    message("Joining Points and Lines")
    joinPoints <- join_points_lines(shade_points, nhdplus_lines)
    
    message("Joining VAA")
    points_vaa <- merge(joinPoints, vaa, by.x = "NHDPlusIDt", by.y = "NHDPlusID", all.x = TRUE) %>%
      st_join(hlr)
    
    message("Calculating BFW")
    output <- mutate(points_vaa, bfwidth = points_vaa$a * points_vaa$TotDASqKM ** points_vaa$b) %>%
      select(site_id, bfwidth, aspect, a, b)
    
    if (use.transect == TRUE) {
      
      message("Calculating Transects")
      
      message("Getting River Lines")
      river_lines <- get_river_lines(gdb_path, nhdplus_lines)
      
      message("Generating Aspects of Line Segments")
      split_lines <- stdh_cast_substring(river_lines, to = "LINESTRING") %>%
        asp_all()
      
      message("Generating Transect Endpoints")
      aspect_points <- join_points_lines(shade_points, split_lines, left.join = FALSE)
      
      message("Creating Transect Lines and Calculating Width")
      transect <- create_transect(aspect_points) %>%
        calc_transect_width(gdb_path)
      
      output$bfwidth[match(transect$site_id, output$site_id)] <- transect$bfwidth
      
      output$aspect[match(transect$site_id, output$site_id)] <- transect$aspect
    }
    
    return(output)
    
    
  }

}


#' Calculates the aspects of lines. 
#' 
#' @description
#' Calculates the angle of an sf linestring relative to north. Returns the full 
#' data frame with an additional column called "aspect".  
#' 
#' @param lines Needs to be a dataframe of class "sf" containing linestring
#' geometry. lines must be in projected in NAD Conus Albers (EPSG:5070).
#'
#' @noRD

asp_all <- function(lines) {

  for (i in 1:nrow(lines)) {
    coords <- st_coordinates(st_geometry(lines)[i])
    x <- as.vector(coords[, 1])
    y <- as.vector(coords[, 2])
    dx <- mean(diff(x))
    dy <- mean(diff(y))
    a <- atan2(dx, dy) * (180 / 3.141593)

    if (a < 0) {
      a <- a + 360
    }

    lines$aspect[i] <- a
  }

  return(lines)
}


#' Breaks line segments with >2 vertices into multiple lines with 2 vertices each. 
#' 
#' @description
#' Takes input lines with >2 vertices and explodes it into individual 2 vertex 
#' line segments. This allows us to find the exact aspect of a river/stream
#' instead of taking the average of a larger section.
#' 
#' @param river_lines Needs to be a dataframe of class "sf" containing linestring
#' geometry. lines must be in projected in NAD Conus Albers (EPSG:5070). In this
#' case, only lines that intersect NHD_Area polygons are used.
#'
#' @noRD


stdh_cast_substring <- function(river_lines, to = "MULTILINESTRING") {
  lines <- st_geometry(river_lines)

  if (!unique(st_geometry_type(lines)) %in% c("POLYGON", "LINESTRING")) {
    stop("Input should be  LINESTRING or POLYGON")
  }
  for (k in 1:length(st_geometry(lines))) {
    sub <- lines[k]
    geom <- lapply(
      1:(length(st_coordinates(sub)[, 1]) - 1),
      function(i) {
        rbind(
          as.numeric(st_coordinates(sub)[i, 1:2]),
          as.numeric(st_coordinates(sub)[i + 1, 1:2])
        )
      }
    ) %>%
      st_multilinestring() %>%
      st_sfc()

    if (k == 1) {
      endgeom <- geom
    } else {
      endgeom <- rbind(endgeom, geom)
    }
  }
  endgeom <- endgeom %>% st_sfc(crs = st_crs(river_lines))
  if (class(river_lines)[1] == "sf") {
    endgeom <- st_set_geometry(river_lines, endgeom)
  }

  if (to == "LINESTRING") {
    endgeom <- endgeom %>% st_cast("LINESTRING")
  }
  return(endgeom)
}


#' Extract NHDPlus HR lines intersect NHD_Area polygons 
#' 
#' @param river_lines Needs to be a dataframe of class "sf" containing linestring
#' geometry. lines must be in projected in NAD Conus Albers (EPSG:5070). In this
#' case, only lines that intersect NHD_Area polygons are used.
#'
#' @noRD


get_river_lines <- function(gdb_path, nhdplus_lines) {
  streams_rivers <- st_read(gdb_path[1], layer = "NHDArea") %>%
    filter(FType == 460) %>%
    st_make_valid() %>%
    st_transform(5070)
  
  if (length(gdb_path) > 1) {
    for (i in 2:length(gdb_path)) {
      streams_rivers2 <- st_read(gdb_path[i], layer = "NHDArea") %>%
        filter(FType == 460) %>%
        st_make_valid() %>%
        st_transform(5070)
      
      streams_rivers <- rbind(streams_rivers, streams_rivers2)
    }
  }
  

  river_lines <- st_filter(nhdplus_lines, streams_rivers, .predicate = st_covered_by)

  return(river_lines)
}


#' Used to create linestrings from the transect coordinates created in `create_transect()`
#' @noRD


st_segment <- function(r) {
  st_linestring(t(matrix(unlist(r), 2, 2)))
}


#' Conversion from degrees to radians
#' @noRD


deg2rad <- function(deg) {
  (deg * 3.141593) / (180)
}

#' Generates transects based on the value in the "aspect" column.  
#' 
#' @param aspect_points Needs to be a dataframe of class "sf" containing point
#' geometry. Points must be in projected in NAD Conus Albers (EPSG:5070). Must
#' have aspect values calculated in a column called "aspect"
#'
#' @param width Width of the transect in meters. Default value is 50.
#' 
#' @noRD


create_transect <- function(aspect_points, width = 50) {
  radius <- width / 2

  coords <- mutate(aspect_points, rev.angle = 360 - aspect - 180) %>%
    mutate(p0.X = st_coordinates(aspect_points)[, 1], p0.Y = st_coordinates(aspect_points)[, 2], p1.X = NA, p1.Y = NA, p2.X = NA, p2.Y = NA) %>%
    mutate(p1.X = (p0.X + (radius * cos(deg2rad(rev.angle)))), p1.Y = (p0.Y + (radius * sin(deg2rad(rev.angle)))), p2.X = (p0.X - (radius * cos(deg2rad(rev.angle)))), p2.Y = (p0.Y - (radius * sin(deg2rad(rev.angle))))) %>%
    st_drop_geometry()

  transect <- data.frame(X = coords$p1.X, Y = coords$p1.Y, PX = coords$p2.X, PY = coords$p2.Y)

  transect$geom <- st_sfc(sapply(1:nrow(transect), function(i) {
    st_segment(transect[i, ])
  }, simplify = FALSE), crs = 5070)

  transect$site_id <- coords$site_id

  transect$aspect <- coords$aspect

  transect <- st_as_sf(transect)

  return(transect)
}

#' Calculates the width of river transects
#' 
#' @description 
#' Takes the transects created in `create_transect()`, clips them using the
#' boundary of NHD_Area, and measures the length. Returns the input data frame 
#' with an updated "bfw" column.
#' 
#' @param transect The transects generated by `create_transect()`. Needs to be a 
#' dataframe of class "sf" containing linestring
#' geometry. Lines must be in projected in NAD Conus Albers (EPSG:5070).
#'
#' @param site_id Name of the unique ID field for the transect lines. Default
#' is "site_id.
#' 
#' @noRD

calc_transect_width <- function(transect, gdb_path, id.field = "site_id") {
  streams_rivers <- st_read(gdb_path[1], layer = "NHDArea") %>%
    filter(FType == 460) %>%
    st_make_valid() %>%
    st_transform(5070)
  
  if (length(gdb_path) > 1) {
    for (i in 2:length(gdb_path)) {
      streams_rivers2 <- st_read(gdb_path[i], layer = "NHDArea") %>%
        filter(FType == 460) %>%
        st_make_valid() %>%
        st_transform(5070)
      
      streams_rivers <- rbind(streams_rivers, streams_rivers2)
    }
  }

  transect$indicator <- st_cast(transect, "MULTIPOINT") %>%
    st_intersects(streams_rivers) %>%
    lengths() > 0

  bfw <- filter(transect, transect$indicator == FALSE) %>%
    st_intersection(streams_rivers)

  test_duplicate <- subset(bfw, duplicated(site_id))

  if (length(test_duplicate$site_id) > 0) {
    bfw <- filter(bfw, !(bfw$site_id %in% test_duplicate$site_id))
  }

  output <- bfw %>%
    select(site_id, aspect) %>%
    mutate(bfwidth = st_length(bfw))

  return(output)
}

#' Extracts the values of horizon angle grids on the locations of the shade points.
#' 
#' @description 
#' At every shade point location, extract the elevation/horizon angle value from
#' the horizon angle grids. In this version, the raster must be organized as follows:
#' * Layer 1: elevation
#' * Layer 2: 0-degree horizon angle
#' * Layer 3: 45-degree horizon angle
#' * Layer 4: 90-degree horizon angle
#' * Layer 5: 135-degree horizon angle
#' * Layer 6: 180-degree horizon angle
#' * Layer 7: 225-degree horizon angle
#' * Layer 8: 270-degree horizon angle
#' * Layer 9: 315-degree horizon angle
#' 
#' In areas where horizon angle grids overlap, the largest value is used. Returns
#' the input with additional columns.
#' 
#' @param points Needs to be a dataframe of class "sf" containing point
#' geometry. Points must be in projected in NAD Conus Albers (EPSG:5070).
#'
#' @param rpu_boundaries must be the provided dataframe of class sf that contains
#' the boundaries of all RPUs and corresponding file names. 

extract_horizon_angle <- function(points, rpu_boundaries) {
  
  rpu <- st_filter(rpu_boundaries, points)
  
  raster_list <- rpu$FILE
  
  for(i in raster_list){
    
    if(file.exists(paste("data/HorizonAngle/", i, sep = "")) == FALSE){
      s3_path <- paste("s3://dmap-epa-prod-anotedata/RShade/Rounded/HorizonAngle/", i, sep = "")
      Sys.setenv(HA = s3_path)
      system("./get_ha.sh")
      
      if(file.exists(paste("data/Elevation/", i, sep = "")) == FALSE){
        s3_path <- paste("s3://dmap-epa-prod-anotedata/RShade/Rounded/Elevation/", i, sep = "")
        Sys.setenv(DEM = s3_path)
        system("./get_dem.sh")
      
     }
    
   }
  

    
  }
  
  
  ha_raster <- rast(paste("data/HorizonAngle/", raster_list[1], sep = ""))
  dem_raster <- rast(paste("data/Elevation/", raster_list[1], sep = ""))

  output <- mutate(points,
    elevation = unlist(terra::extract(dem_raster, st_coordinates(points), list = TRUE)),
    topoNN = unlist(terra::extract(ha_raster[[1]], st_coordinates(points), list = TRUE)),
    topoNE = unlist(terra::extract(ha_raster[[2]], st_coordinates(points), list = TRUE)),
    topoEE = unlist(terra::extract(ha_raster[[3]], st_coordinates(points), list = TRUE)),
    topoSE = unlist(terra::extract(ha_raster[[4]], st_coordinates(points), list = TRUE)),
    topoSS = unlist(terra::extract(ha_raster[[5]], st_coordinates(points), list = TRUE)),
    topoSW = unlist(terra::extract(ha_raster[[6]], st_coordinates(points), list = TRUE)),
    topoWW = unlist(terra::extract(ha_raster[[7]], st_coordinates(points), list = TRUE)),
    topoNW = unlist(terra::extract(ha_raster[[8]], st_coordinates(points), list = TRUE))
  )
  if (length(raster_list) > 1) {
    for (i in 2:length(raster_list)) {
      ha_raster <- rast(paste("data/HorizonAngle/", raster_list[i], sep = ""))
      dem_raster <- rast(paste("data/Elevation/", raster_list[i], sep = ""))
      message("Evaluating Additional Grids...")
      output2 <- mutate(points,
        elevation = unlist(terra::extract(dem_raster, st_coordinates(points), list = TRUE)),
        topoNN = unlist(terra::extract(ha_raster[[1]], st_coordinates(points), list = TRUE)),
        topoNE = unlist(terra::extract(ha_raster[[2]], st_coordinates(points), list = TRUE)),
        topoEE = unlist(terra::extract(ha_raster[[3]], st_coordinates(points), list = TRUE)),
        topoSE = unlist(terra::extract(ha_raster[[4]], st_coordinates(points), list = TRUE)),
        topoSS = unlist(terra::extract(ha_raster[[5]], st_coordinates(points), list = TRUE)),
        topoSW = unlist(terra::extract(ha_raster[[6]], st_coordinates(points), list = TRUE)),
        topoWW = unlist(terra::extract(ha_raster[[7]], st_coordinates(points), list = TRUE)),
        topoNW = unlist(terra::extract(ha_raster[[8]], st_coordinates(points), list = TRUE))
      )

      output$elevation <- coalesce(output$elevation, output2$elevation)
      output$topoNN <- coalesce(output$topoNN, output2$topoNN)
      output$topoNE <- coalesce(output$topoNE, output2$topoNE)
      output$topoEE <- coalesce(output$topoEE, output2$topoEE)
      output$topoSE <- coalesce(output$topoSE, output2$topoSE)
      output$topoSS <- coalesce(output$topoSS, output2$topoSS)
      output$topoSW <- coalesce(output$topoSW, output2$topoSW)
      output$topoWW <- coalesce(output$topoWW, output2$topoWW)
      output$topoNW <- coalesce(output$topoNW, output2$topoNW)

      output <- mutate(output,
        elevation = case_when(elevation < output2$elevation ~ output2$elevation, TRUE ~ elevation),
        topoNN = case_when(topoNN < output2$topoNN ~ output2$topoNN, TRUE ~ topoNN),
        topoNE = case_when(topoNE < output2$topoNE ~ output2$topoNE, TRUE ~ topoNE),
        topoEE = case_when(topoEE < output2$topoEE ~ output2$topoEE, TRUE ~ topoEE),
        topoSE = case_when(topoSE < output2$topoSE ~ output2$topoSE, TRUE ~ topoSE),
        topoSS = case_when(topoSS < output2$topoSS ~ output2$topoSS, TRUE ~ topoSS),
        topoSW = case_when(topoSW < output2$topoSW ~ output2$topoSW, TRUE ~ topoSW),
        topoWW = case_when(topoWW < output2$topoWW ~ output2$topoWW, TRUE ~ topoWW),
        topoNW = case_when(topoNW < output2$topoNW ~ output2$topoNW, TRUE ~ topoNW)
      )
    }
  }

  return(output)
}


#' Extracts the values of a grid using a multi-angle, multi-distance sampling 
#' method from each input point.
#' 
#' @description 
#' For every input point, extract the value at a given angle and distance away
#' and place that value in a column within the input point data frame. This is
#' done for every angle and distance combination. 
#' 
#' @param points Needs to be a dataframe of class "sf" containing point
#' geometry. Points must be in projected in NAD Conus Albers (EPSG:5070).
#'
#' @param veg_raster Path to the raster file you will be using.
#' 
#' @param angle_list Vector containing the angles the sample points will be 
#' generated at. Angles can be removed, but angles outside of the defaul 8 angles
#' are not valid. 
#' 
#' @param dist_list Vector containing the sample point distances from the origin.
#' 
#' @param type The middle part of the column name. Vegetation hight is "hght,
#' and vegetation cover is "cvr". Default value is "hght".

star_sample <- function(points, veg_raster, angle_list, dist_list, type = "hght") {
  veg_sample <- points

  for (i in dist_list) {
    for (j in angle_list) {
      shade_coords <- st_coordinates(points)

      if (j == 0) {
        shade_coords[, 2] <- shade_coords[, 2] + (i + (points$bfwidth / 2))
        direction <- "NN"
      }
      if (j == 45) {
        shade_coords[, 1] <- shade_coords[, 1] + ((i + (points$bfwidth / 2)) * sqrt(2))
        shade_coords[, 2] <- shade_coords[, 2] + ((i + (points$bfwidth / 2)) * sqrt(2))
        direction <- "NE"
      }
      if (j == 90) {
        shade_coords[, 1] <- shade_coords[, 1] + (i + (points$bfwidth / 2))
        direction <- "EE"
      }
      if (j == 135) {
        shade_coords[, 1] <- shade_coords[, 1] + ((i + (points$bfwidth / 2)) * sqrt(2))
        shade_coords[, 2] <- shade_coords[, 2] - ((i + (points$bfwidth / 2)) * sqrt(2))
        direction <- "SE"
      }
      if (j == 180) {
        shade_coords[, 2] <- shade_coords[, 2] - (i + (points$bfwidth / 2))
        direction <- "SS"
      }
      if (j == 225) {
        shade_coords[, 1] <- shade_coords[, 1] - ((i + (points$bfwidth / 2)) * sqrt(2))
        shade_coords[, 2] <- shade_coords[, 2] - ((i + (points$bfwidth / 2)) * sqrt(2))
        direction <- "SW"
      }
      if (j == 270) {
        shade_coords[, 1] <- shade_coords[, 1] - (i + (points$bfwidth / 2))
        direction <- "WW"
      }
      if (j == 315) {
        shade_coords[, 1] <- shade_coords[, 1] - ((i + (points$bfwidth / 2)) * sqrt(2))
        shade_coords[, 2] <- shade_coords[, 2] + ((i + (points$bfwidth / 2)) * sqrt(2))
        direction <- "NW"
      }

      colname <- paste(direction, type, i, sep = "")

      veg_sample <- mutate(veg_sample, !!colname := unlist(terra::extract(veg_raster, shade_coords, list = TRUE)))
    }
  }

  return(veg_sample)
}


#' Adds latitute and longitude columns to a shade sample points dataframe.
#' 
#' @description 
#' Takes a shade points dataframe and calculates the latitude and longitude based
#' in NAD83 (EPSG: 4269)

#' @param points Needs to be a dataframe of class "sf" containing point
#' geometry. Points must be in projected in NAD Conus Albers (EPSG:5070).


add_latlon <- function(points) {
  nad83Points <- st_transform(points, 4269)

  coords <- st_coordinates(nad83Points)

  output <- mutate(points, lat = coords[, 2], lon = coords[, 1])

  return(output)
}


#' Copies bankfull width values from a table and uses these values in lieu of 
#' calculating them using equations of transect lenghts. 
#' 
#' @description 
#' Takes either an excel table or csv and matches the bankfull width values based
#' on NHDPlusIDt. Values are then copied into the bfwidth column of your RShade 
#' points. 
#' 
#' @param points Needs to be a dataframe of class "sf" containing point
#' geometry. Points must be in projected in NAD Conus Albers (EPSG:5070).
#' 
#' @param bfw_table Needs to be either a csv or xlsx. Must contain at least 2
#' columns: One column called NHDPlusIDt containing the ID values as text and one
#' column called 'bfwidth' containing the values for bankfull width in meters.

copy_BFW <- function(points, bfw_table){
  
  message("Reading BFW Table")
  if(tools::file_ext(bfw_table) == "xls" | tools::file_ext(bfw_table) == "xlsx"){
    message("Excel file detected")
    bfw <- read_excel(bfw_table)
  } else if(tools::file_ext(bfw_table) == "csv"){
    message("CSV detected")
    bfw <- read.csv(bfw_table,  colClasses=c("NHDPlusIDt"="character"))
  } else{
    stop("Failed to read BFW Table. File must be .xls, .xlsx, or .csv.")
  }
  
  output <- merge(points, bfw, "NHDPlusIDt", all.x = TRUE) %>%
    select(site_id, bfwidth, aspect)
}


#' Extracts the values of a grid using a multi-angle, multi-distance sampling 
#' method from each input point. This is the same process as `star_sample()`, 
#' but with parallel processing implemented to decrease processing time. 
#' 
#' @description 
#' For every input point, extract the value at a given angle and distance away
#' and place that value in a column within the input point data frame. This is
#' done for every angle and distance combination. 
#' 
#' @param points Needs to be a dataframe of class "sf" containing point
#' geometry. Points must be in projected in NAD Conus Albers (EPSG:5070).
#'
#' @param veg_raster Path to the raster file you will be using.
#' 
#' @param angle_list Vector containing the angles the sample points will be 
#' generated at. Angles can be removed, but angles outside of the defaul 8 angles
#' are not valid. 
#' 
#' @param dist_list Vector containing the sample point distances from the origin.
#' 
#' @param type The middle part of the column name. Vegetation hight is "hght,
#' and vegetation cover is "cvr". Default value is "hght".

star_sample_parallel <- function(points, veg_raster, angle_list, dist_list, type = "hght") {
  
  sample_list <- mclapply(dist_list, function(x){
    
    star_sample_points <- star_sample(points, veg_raster, angle_list, x, type)
    
  }, mc.cores = 4)
  
  for(i in 2:length(sample_list)){
    
    sample_list[[1]] <- mutate(sample_list[[1]], sample_list[[i]])
  }
  
  result <- sample_list[[1]]
  
  return(result)
}
